[
  {
    "objectID": "bandits_notes.html",
    "href": "bandits_notes.html",
    "title": "Multi Armed Bandits 101",
    "section": "",
    "text": "This document can be considered as a proto-lecture notes for bandit algorithms. It is designed to be an introduction to simple k-armed bandits to teach the fundamentals. Content here can be a lecture worth of a 1 to 3 full-day workshop.\nWhile the code is in Python, the process will be simple enough that procedures here can even be implemented with Excel formulas to a good degree."
  },
  {
    "objectID": "bandits_notes.html#environment",
    "href": "bandits_notes.html#environment",
    "title": "Multi Armed Bandits 101",
    "section": "Environment",
    "text": "Environment\nIn simple k-armed bandit problems, the environment is your arms on the bandit. At each iteration, one of the arms is selected and the arm yields a “reward”. Agents (decision makers) are not supposed to know the logic behind the reward process, otherwise finding the best arm is a trivial task. Though we may need to design bandits in our experiments.\nA very simple arm design is a uniform random variate generator with lowest value \\(a\\) and highest value \\(b\\). Expected reward (\\(E[R]\\)) from that arm will therefore be \\(E[R] = (a+b)/2\\). Suppose we are dealing with two arms with parameters \\((a=2,b=5)\\) and \\((a=2.5,b=4.7)\\). So, expected rewards are \\(E[R_1] = 3.5\\) and \\(E[R_2] = 3.6\\). Even though Arm 1 have a higher high-end (5 vs 4.7), on average Arm 2 is a better bet (3.6 vs 3.5).\nBandits more complex than a stationary distribution are out of the core scope of this lecture. They can be mentioned in discussions or some variants can be introduced as extra exercises."
  },
  {
    "objectID": "bandits_notes.html#agent",
    "href": "bandits_notes.html#agent",
    "title": "Multi Armed Bandits 101",
    "section": "Agent",
    "text": "Agent\nAgent is the decision maker. Agents makes their decisions (actions) according to a policy. This policy makes use of starting assumptions and consequences of its actions (i.e. rewards). An action in simple k-armed bandit problems is simply choosing an arm.\nAgents generally keep record of expected reward estimates of arms and update those estimates as they get the result of their actions. Reward estimates are input to policy functions and policies result in decisions (i.e. next actions).\nInitially (i.e. \\(t=0\\)), all expected reward estimates are assumed to be zero. However this assumption may be adjusted (i.e. optimistic greedy policy)."
  },
  {
    "objectID": "bandits_notes.html#interaction",
    "href": "bandits_notes.html#interaction",
    "title": "Multi Armed Bandits 101",
    "section": "Interaction",
    "text": "Interaction\nIn simple k-armed bandit problem, interaction between the agent and the environment is limited to one action per time step. Agent chooses an arm and receives its reward, that’s it. No multiple choices or no extras (e.g. budgets) at the start. Time is described as discrete (i.e. time step). At each time step the agent decides on a single arm."
  },
  {
    "objectID": "bandits_notes.html#greedy-and-epsilon-greedy",
    "href": "bandits_notes.html#greedy-and-epsilon-greedy",
    "title": "Multi Armed Bandits 101",
    "section": "Greedy and Epsilon Greedy",
    "text": "Greedy and Epsilon Greedy\nGreedy always picks the arm with the highest estimated reward. If we add a small probability (hence \\(\\epsilon\\)) to break habit and pick entirely randomly then it is \\(\\epsilon\\)-Greedy.\nWe can almost always say Greedy will stuck at local optima (i.e. nice way of saying “It could be better.”). Main reason is finding the best arm severely depends on early actions and their rewards. Suppose there are two bandits. First bandit yields either of \\((-1,7)\\) with equal probability and the second bandit yields \\((1,2)\\). We know that first bandit’s actual expected reward (\\(q_*(a=1) = -1 *0.5 + 7*0.5 = 3\\)) is higher than the second one (\\(q_*(a=2) = 1 *0.5 + 2*0.5 = 1.5\\)). Our reward estimates for the first step is 0 for both bandits (\\(Q_1(a=1) = 0, Q_1(a=2) = 0\\)).\nBut, our greedy agent does not know this fact so it begins to explore. Suppose we pick our first action randomly (\\(A_1 = 1\\)) and reward becomes (\\(R_1 = -1\\)). We update our estimates for the next step to (\\(Q_2(a=1) = -1, Q_2(a=2) = 0\\)). So by Greedy logic, our next action is (\\(A_2 = 2\\)), and suppose its outcome is (\\(R_2 = 1\\)). We update our estimates to (\\(Q_3(a=1) = -1, Q_3(a=2) = 1\\)). The preference is again for the second bandit. Suppose this time reward is (\\(R_3 = 2\\)), therefore estimates are updated (\\(Q_4(a=1) = -1, Q_4(a=2) = (1 + 2)/2 = 1.5\\)). Due to an unfortunate outcome in the first step, first bandit will never be selected in this setting again with a greedy approach.\n\nEpsilon (\\(\\epsilon\\)) Approach\nGreedy approaches are pure “exploitation”. Good thing about exploitation is, if we are on the right track we can improve quickly. Bad news is we are seldom on the right track but more frequently we are stuck in some place we think we are on the right track. \\(\\epsilon\\)-Greedy is a solution to add some “exploration”. When we set \\(\\epsilon\\) to a small probability (say, 0.1), we simply encourage the algorithm to try different alternatives and hopefully be more serendipitous. Exploration vs exploitation is an essential topic that should be discussed more.\nSuppose at the end of time step 7, (\\(Q_8(a=1) = -1, Q_8(a=2) = 1.62\\)), epsilon kicks in and \\(A_8 = 1\\). This time \\(R_8 = 7\\) and estimated rewards are updated to (\\(Q_9(a=1) = 3, Q_9(a=2) = 1.62\\)).\n\n\nOptimistic Initial Values\nWe had a simple assumption of \\(Q_1(a) = 0, \\forall_a\\). What if we start with an optimistic value of 10? (\\(Q_1(a) = 10, \\forall_a\\)). Then Greedy algorithm will need to explore at the first iterations. It might improve the performance of the algorithm but there are occasion which this method is ineffective."
  },
  {
    "objectID": "bandits_notes.html#upper-confidence-bound-ucb",
    "href": "bandits_notes.html#upper-confidence-bound-ucb",
    "title": "Multi Armed Bandits 101",
    "section": "Upper Confidence Bound (UCB)",
    "text": "Upper Confidence Bound (UCB)\nAnother approach to remedy shortsightedness of Greedy method is to add a “bonus” of what the outcome might be. UCB formulates this bonus as an upper (confidence) bound given below. \\(c\\) is a parameter to be chosen, while \\(\\ln t\\) represents natural logarithm of the current time step \\(t\\) and \\(N_t(a)\\) is the number of times that action \\(a\\) is chosen.\n\\[A_t = \\underset{a}{\\mathrm{argmax}} \\left[ Q_t(a) + c \\sqrt{\\dfrac{\\ln t}{N_t(a)}} \\right]\\] The resulting extra (\\(c \\sqrt{\\dfrac{\\ln t}{N_t(a)}}\\)) is added to agent’s reward estimation for that action (\\(Q_t(a)\\)) to improve the chances of an action to be taken. If (\\(N_t(a) = 0\\)), then the action’s estimated reward goes to infinity therefore makes the action a priority.\nUpper confidence bound encourages exploration but converges to exploitation as the bandit is explored more frequently. The logic behind can be similar to the thought “This bandit is expected to perform X but it might not be explored that much, so let’s make it X + Y, which Y gets smaller as we visit that bandit.”"
  },
  {
    "objectID": "bandits_notes.html#gradient-bandits",
    "href": "bandits_notes.html#gradient-bandits",
    "title": "Multi Armed Bandits 101",
    "section": "Gradient Bandits",
    "text": "Gradient Bandits\nGradient bandits differ from other algorithms by implementing a preference scheme rather than using reward estimates directly. Denote preference variable with \\(H_t(a)\\). Then, instead of selecting with the higher preference convert preference values (scores) to probabilities using the formula below.\n\\[Pr\\{A_t = a\\} = \\pi_t(a) = \\dfrac{e^{H_t(a)}}{\\sum_{b=1}^ke^{H_t(b)}}\\]\nThen at each step, update the preference values (\\(H(A_t)\\)).\n\\(H_{t+1}(A_t) = H_{t}(A_t) + \\alpha(R_t-\\bar{R_t})(1-\\pi_t(A_t))\\) if \\(A_t = a\\), and\n\\(H_{t+1}(A_t) = H_{t}(A_t) - \\alpha(R_t-\\bar{R_t})\\pi_t(A_t), \\forall_{A_t \\neq a}\\)\nwhere \\(\\alpha\\) is the time step, \\(\\bar{R_t}\\) is the average reward up to but not including time \\(t\\), also called “the baseline”."
  },
  {
    "objectID": "bandits_notes.html#thompson-sampling",
    "href": "bandits_notes.html#thompson-sampling",
    "title": "Multi Armed Bandits 101",
    "section": "Thompson Sampling",
    "text": "Thompson Sampling\nThe final fundamental algorithm for MAB comes from Thompson’s work, which is actually from 1930s. While the approach was largely ignored for about 80 years it is now considered as one of the most successful algorithms for MAB work. I refer to the tutorial given in the References while adding some more elaborations.\nThompson Sampling is considered under “Bayesian Bandits” category but we are not going to get into details of prior/posterior distributions. Just keep in mind that when someone uses Bayes term, it is about extra (given) information that manipulates probabilities.\n\nBeta-Bernoulli Bandits\nFor the sake of simplicity we will constrain ourselves to the “Beta-Bernoulli Bandit” Example. A Bernoulli Bandit means that an event happens with probability \\(p\\) and does not happen with probability \\(q = 1-p\\), just like a coin toss (for the coin toss \\(p=q=0.5)\\). So, our Bernoulli bandits give us a reward (say, “success”) with a probability \\(p_k\\) for the \\(k\\)-th bandit and nothing (“fail”) with \\(q_k\\). The average (mean value) of the reward is simply \\(E[A_k] = \\theta_k = p_k\\) and our estimate for \\(\\theta_k\\) is simply denoted as \\(\\hat{\\theta_k}\\).\n\n\nBeta Distribution\nHere is the part that gets slightly complicated. Our assumptions about \\(\\theta_k\\) is based on Beta distribution. Beta distribution is controlled by two shape parameters (\\(\\alpha\\),\\(\\beta\\)). Quantile values of the Beta distribution are always between 0 and 1.\nProbability density function seems a bit ominous for the uninitiated but it is fine. Gamma function converts to factorial for positive integer values (\\(\\Gamma(x) = (x-1)!\\)) and we will be working on positive integer values for this example. In additon many programming languages have built-in functions to handle Beta distribution related operations.\n\\[P(\\theta_k;\\alpha_k,\\beta_k) =\\dfrac{\\Gamma(\\alpha_k + \\beta_k)}{\\Gamma(\\alpha_k)\\Gamma(\\beta_k)}\\theta_k^{\\alpha_k-1}(1-\\theta_k)^{\\beta_k-1}\\]\n\n\nHow it works\nLet’s put it in some context. Suppose we are showing an ad on a webpage. Each visitor arrives and sees the ad (“view”) and there is a chance to click (“click”) on that ad. We measure success with clicks (\\(\\alpha\\) parameter) and failures with views - clicks (\\(\\beta\\) parameter). Finally \\(\\theta\\) is simply clicks/views. It is also called Clickthrough Rate (CTR).\nRecall the greedy algorithm as it considers highest success proportion (\\(\\hat{\\theta_k}=\\alpha_k/(\\alpha_k+\\beta_k)\\)) as the next ad to show. In mathematical terms \\(A_{t+1} = \\mathrm{argmax}_k \\{\\hat{\\theta_k}\\}\\).\nThompson Sampling algorithm does exactly the same except it samples mean value from Beta distribution (\\(\\hat{\\theta_k} \\sim beta(\\alpha_k,\\beta_k)\\)). This way, Thompson Sampling provides some degree of tolerance for underperforming but infrequently tested alternatives. Recall upper bound in UCB also boosted relatively infrequently tried alternatives. The best part is as the sample grows at each iteration, the algorithm practically gives its verdict on the alternatives and it is consistent on the preferences."
  },
  {
    "objectID": "intro_ml_2.html",
    "href": "intro_ml_2.html",
    "title": "Statistical Models in R: Part 2",
    "section": "",
    "text": "This is part of old lecture notes\nThis part explains classification and regression trees, following the work of Breiman et al. (there is a copy at the library). Although there are other and improved versions, we cover CART because it is important to understand fundamentals. The R package rpart also follows the same study to construct trees.\nWe are going to use the following packages. Install those packages (use install.packages(\"packagenamehere\")), if you haven’t already.\n\nlibrary(tidyverse) #For data manipulation\nlibrary(rpart) #To construct CART models\nlibrary(rpart.plot) # It also includes titanic data\nlibrary(rattle) #For visualization\n\n\nExample Problem (Titanic Deaths/Survivors)\nThe problem Suppose we would like to correctly identify each survivor in Titanic accident just by looking at some traits such as passenger class, sex and age.\nBonus: A version of this example can be found on Kaggle as a tutorial! Check it out from this link.\n\nset.seed(58) #Set the random seed\ndata(ptitanic) #Call the data\n\ntitanic_data &lt;-\nptitanic %&gt;%\nselect(survived,pclass,sex,age) %&gt;% #Select only required columns\nmutate(age=as.numeric(age)) %&gt;% #This is a fix, just for this data set\nfilter(complete.cases(.)) %&gt;% #Remove all NA including rows\n#Split the data into train and test\nmutate(train_test = ifelse(runif(nrow(.)) &lt; 0.25,\"test\",\"train\")) %&gt;%\ntbl_df()\n\nWarning: `tbl_df()` was deprecated in dplyr 1.0.0.\nℹ Please use `tibble::as_tibble()` instead.\n\nprint(titanic_data)\n\n# A tibble: 1,046 × 5\n   survived pclass sex       age train_test\n   &lt;fct&gt;    &lt;fct&gt;  &lt;fct&gt;   &lt;dbl&gt; &lt;chr&gt;     \n 1 survived 1st    female 29     train     \n 2 survived 1st    male    0.917 test      \n 3 died     1st    female  2     train     \n 4 died     1st    male   30     train     \n 5 died     1st    female 25     train     \n 6 survived 1st    male   48     train     \n 7 survived 1st    female 63     test      \n 8 died     1st    male   39     train     \n 9 survived 1st    female 53     train     \n10 died     1st    male   71     train     \n# … with 1,036 more rows\n\n#Build the model with the training data\ntitanic_train &lt;- titanic_data %&gt;% filter(train_test == \"train\") %&gt;% select(-train_test)\ntitanic_model &lt;- rpart(survived ~ ., data=titanic_train)\nfancyRpartPlot(titanic_model)\n\n\n\n\n\n\nClassification and Regression Trees (CART)\nCART algorithm is a very convenient and easy to interpret method to partition data in a meaningful way with binary splits (i.e. \\(x &gt; A\\) and \\(x \\le A\\)). Both regression and classification trees aim to minimize the error (misclassification).\nError rate for regression trees is the squared distance (very similar to linear regression) of each item in the node to the average response (\\(\\hat{Y_k}\\)) value of the node. Suppose there are \\(K\\) final nodes, each group’s response variable is defined as \\(\\hat{Y_k} = \\dfrac{1}{N_k}\\sum_{i \\in G_k} y_i\\). So the final objective function to minimize is as follows.\n\\[\\min \\sum_{k}^K (\\min_{i \\in G_k} (y_i - \\hat{Y_k})^2) + \\alpha |K| \\]\nThe complexity parameter \\(\\alpha\\) is used as a balancer between the number of final nodes and total error. If \\(\\alpha = 0\\) then each node will consist of a single item to minimize error.\nIn classification, it is a bit different. For the sake of simplicity, assume binary classification (0 or 1). A node’s class (C_k) is determined by majority. \\(C_k = \\argmax_{0,1}\\{N_{0,k}, N_{1,k}\\}\\). We also calculate the quality (or probability) of correct classification with \\(\\hat{p}_{(C_k,k)} = \\dfrac{1}{N_k} \\sum_{i \\in G_k} I(y_i = C_k)\\) (proportion of correct class items). For binary clasification we can simply say \\(\\hat{p}_k\\).\nThere are three types of error: misclassification, Gini index and cross-entropy.\n\nMisclassification Error: \\(1 - \\hat{p}_k\\). Just proportion of the wrong assignment.\nGini Index: \\(2\\hat{p}_k(1-\\hat{p}_k)\\).\nCross-entropy: \\(-\\hat{p}_k \\log(\\hat{p}_k) - (1-\\hat{p}_k)\\log(1-\\hat{p}_k)\\)\n\nps. rpart uses Gini index to measure error.\n\n\nOut-of-sample Analysis\nFitting the model and getting good results is fine. But, if it cannot predict response values outside the training data, then your model overfits.\n\n## In sample prediction classification results\ntitanic_in_sample &lt;- predict(titanic_model)\n\nprint(head(titanic_in_sample))\n\n        died  survived\n1 0.06358382 0.9364162\n2 0.06358382 0.9364162\n3 0.83690987 0.1630901\n4 0.06358382 0.9364162\n5 0.83690987 0.1630901\n6 0.83690987 0.1630901\n\nin_sample_prediction &lt;-\ncbind(\n    titanic_in_sample %&gt;% tbl_df %&gt;%\n        transmute(survived_predict = ifelse(survived &gt;= 0.5,1,0)),\n    titanic_train %&gt;% tbl_df %&gt;%\n        transmute(survived_actual = ifelse(survived == \"survived\",1,0))\n) %&gt;%\nmutate(correct_class = (survived_predict == survived_actual)) %&gt;%\ngroup_by(correct_class) %&gt;%\nsummarise(count=n(),percentage=n()/nrow(.))\n\nprint(in_sample_prediction)\n\n# A tibble: 2 × 3\n  correct_class count percentage\n  &lt;lgl&gt;         &lt;int&gt;      &lt;dbl&gt;\n1 FALSE           146      0.184\n2 TRUE            646      0.816\n\ntitanic_test &lt;- titanic_data %&gt;% filter(train_test==\"test\") %&gt;% select(-train_test)\ntitanic_predict &lt;- predict(titanic_model,newdata=titanic_test)\nprint(head(titanic_predict))\n\n        died  survived\n1 0.00000000 1.0000000\n2 0.06358382 0.9364162\n3 0.83690987 0.1630901\n4 0.83690987 0.1630901\n5 0.83690987 0.1630901\n6 0.06358382 0.9364162\n\nout_of_sample_prediction &lt;-\ncbind(\n    titanic_predict %&gt;% tbl_df %&gt;%\n        transmute(survived_predict = ifelse(survived &gt;= 0.5,1,0)),\n    titanic_test %&gt;% tbl_df %&gt;%\n        transmute(survived_actual = ifelse(survived == \"survived\",1,0))\n) %&gt;%\nmutate(correct_class = (survived_predict == survived_actual)) %&gt;%\ngroup_by(correct_class) %&gt;%\nsummarise(count=n(),percentage=n()/nrow(.))\n\nprint(out_of_sample_prediction)\n\n# A tibble: 2 × 3\n  correct_class count percentage\n  &lt;lgl&gt;         &lt;int&gt;      &lt;dbl&gt;\n1 FALSE            56      0.220\n2 TRUE            198      0.780\n\n\n\n\nModel Parameters\nYou can improve the model with these extra properties.\n\n    rpart.control(\n        minsplit = 20, #Min # of items that should be in a node to do a split\n        minbucket = round(minsplit/3), #Minimum number of items in a final node\n        cp = 0.01, #Complexity parameter (min improvement to generate a split)\n        maxcompete = 4, #Not related to model. Some printout for analyses\n        maxsurrogate = 5, #Used to deal with missing values\n        usesurrogate = 2, #Used to deal with missing values\n        xval = 10, #Number of cross validations\n        surrogatestyle = 0, #Used to deal with missing values\n        maxdepth = 30 #Tree depth\n    )\n\n\n\nComparison with Logistic Regression\n\ntitanic_logit_model &lt;- glm(survived ~ ., data=titanic_train,family=binomial(link = \"logit\"))\ntitanic_probit_model &lt;- glm(survived ~ ., data=titanic_train,family=binomial(link = \"probit\"))\n\ntitanic_logit_in_sample &lt;- predict(titanic_logit_model,type=\"response\")\n\ntitanic_logit_in_sample_prediction &lt;-\ndata.frame(in_sample=(titanic_logit_in_sample &gt;= 0.5)*1,\n            actual=(titanic_train$survived == \"survived\")*1) %&gt;%\n            mutate(correct_class= (in_sample == actual)) %&gt;%\n            group_by(correct_class) %&gt;%\n            summarise(count=n(),percentage=n()/nrow(.))\n\n\nprint(titanic_logit_in_sample_prediction)\n\n# A tibble: 2 × 3\n  correct_class count percentage\n  &lt;lgl&gt;         &lt;int&gt;      &lt;dbl&gt;\n1 FALSE           169      0.213\n2 TRUE            623      0.787\n\ntitanic_logit_out_of_sample &lt;- predict(titanic_logit_model,newdata=titanic_test,type=\"response\")\n\ntitanic_logit_out_of_sample_prediction &lt;-\ndata.frame(out_of_sample=(titanic_logit_out_of_sample &gt;= 0.5)*1,\n            actual=(titanic_test$survived == \"survived\")*1) %&gt;%\n            mutate(correct_class= (out_of_sample == actual)) %&gt;%\n            group_by(correct_class) %&gt;%\n            summarise(count=n(),percentage=n()/nrow(.))\n\nprint(titanic_logit_out_of_sample_prediction)\n\n# A tibble: 2 × 3\n  correct_class count percentage\n  &lt;lgl&gt;         &lt;int&gt;      &lt;dbl&gt;\n1 FALSE            59      0.232\n2 TRUE            195      0.768\n\ntitanic_probit_in_sample &lt;- predict(titanic_probit_model,type=\"response\")\n\ntitanic_probit_in_sample_prediction &lt;-\ndata.frame(in_sample=(titanic_probit_in_sample &gt;= 0.5)*1,\n            actual=(titanic_train$survived == \"survived\")*1) %&gt;%\n            mutate(correct_class= (in_sample == actual)) %&gt;%\n            group_by(correct_class) %&gt;%\n            summarise(count=n(),percentage=n()/nrow(.))\n\n\nprint(titanic_probit_in_sample_prediction)\n\n# A tibble: 2 × 3\n  correct_class count percentage\n  &lt;lgl&gt;         &lt;int&gt;      &lt;dbl&gt;\n1 FALSE           166      0.210\n2 TRUE            626      0.790\n\ntitanic_probit_out_of_sample &lt;- predict(titanic_probit_model,newdata=titanic_test,type=\"response\")\n\ntitanic_probit_out_of_sample_prediction &lt;-\ndata.frame(out_of_sample=(titanic_probit_out_of_sample &gt;= 0.5)*1,\n            actual=(titanic_test$survived == \"survived\")*1) %&gt;%\n            mutate(correct_class= (out_of_sample == actual)) %&gt;%\n            group_by(correct_class) %&gt;%\n            summarise(count=n(),percentage=n()/nrow(.))\n\n\nprint(titanic_probit_out_of_sample_prediction)\n\n# A tibble: 2 × 3\n  correct_class count percentage\n  &lt;lgl&gt;         &lt;int&gt;      &lt;dbl&gt;\n1 FALSE            60      0.236\n2 TRUE            194      0.764\n\n\n\n\nBenchmarking\nLet’s see all analyses together. Keep in mind that we did not do any external cross-validation (it would be good exercise).\n\ncomplete_benchmark &lt;-\ndata.frame(\n    model = c(\"CART\",\"Logistic Reg. - Logit Link\",\n            \"Logistic Reg. - Probit Link\"),\n    in_sample_accuracy = c(\n        in_sample_prediction %&gt;%\n            filter(correct_class) %&gt;%\n            transmute(round(percentage,4)) %&gt;%\n            unlist(),\n        titanic_logit_in_sample_prediction %&gt;%\n            filter(correct_class) %&gt;%\n            transmute(round(percentage,4)) %&gt;%\n            unlist(),\n        titanic_probit_in_sample_prediction %&gt;%\n            filter(correct_class) %&gt;%\n            transmute(round(percentage,4)) %&gt;%\n            unlist()\n    ),\n    out_of_sample_accuracy = c(\n        out_of_sample_prediction %&gt;%\n            filter(correct_class) %&gt;%\n            transmute(round(percentage,4)) %&gt;%\n            unlist(),\n        titanic_logit_out_of_sample_prediction %&gt;%\n            filter(correct_class) %&gt;%\n            transmute(round(percentage,4)) %&gt;%\n            unlist(),\n        titanic_probit_out_of_sample_prediction %&gt;%\n            filter(correct_class) %&gt;%\n            transmute(round(percentage,4)) %&gt;%\n            unlist()\n    )\n\n    )\n\nprint(complete_benchmark)\n\n                        model in_sample_accuracy out_of_sample_accuracy\n1                        CART             0.8157                 0.7795\n2  Logistic Reg. - Logit Link             0.7866                 0.7677\n3 Logistic Reg. - Probit Link             0.7904                 0.7638\n\n\n\n\nReferences\n\nThese lecture notes were initially used at Bogazici University Engineering Management Master Program."
  },
  {
    "objectID": "intro.html#dersin-amaçları",
    "href": "intro.html#dersin-amaçları",
    "title": "Tanışma ve Giriş",
    "section": "Dersin amaçları",
    "text": "Dersin amaçları\nBu dersin üç ana amacı bulunmaktadır.\n\nTemel veri analiz ve veri bilimi yetkinliklerini aktarmak\nVeri bilimi ile ilgili ileri düzey çalışmalar konusunda bilgilendirmek\nKatılımcıların bu derste gördükleri ve öğrendiklerini kendi kurumlarındaki süreçlere çevik bir şekilde entegre edebilmelerini sağlamak\n\nBu amaçlara ulaşmak doğrultusunda her hafta\n\nVeri bilimi konusunda teknik bir konu işlenecektir\nSektörden misafir hocalar ile farklı sektörlerdeki ileri düzey konular hakkında bilgilendirme sağlanacaktır\nDers içi uygulamalar ile öğrenilenler pekiştirilecektir\n\nDersin sonunda öğrencilerin iş hayatlarında veri bilimi projelerini uygulayabilecekleri bir çerçeve yöntemin nosyonunu ve ihtiyaç duydukları gerekli araçları nasıl kullanabileceklerini öğrenmeleri hedeflenmektedir."
  },
  {
    "objectID": "intro.html#ders-programı",
    "href": "intro.html#ders-programı",
    "title": "Tanışma ve Giriş",
    "section": "Ders programı",
    "text": "Ders programı\nHaftalık tahmini program aşağıdaki gibidir. Değişiklikler olabilir.\n\nTanışma ve veri analitiğine giriş\nVeri düzenleme ve görselleştirme\nOtomatik Raporlama\nModelleme (Optimizasyon / Makine Öğrenmesi / Yapay Zeka)\nÜrün Geliştirme\nBulut Bilişim\nSunumlar\nhafta ve 6. hafta arasında dersimize misafir hocaların katılması hedeflenmektedir."
  },
  {
    "objectID": "intro.html#derse-katılım-ve-değerlendirme",
    "href": "intro.html#derse-katılım-ve-değerlendirme",
    "title": "Tanışma ve Giriş",
    "section": "Derse Katılım ve Değerlendirme",
    "text": "Derse Katılım ve Değerlendirme\nDerse katılım Zoom üzerinden gerçekleştirilecektir.\nDeğerlendirme için aşağıdaki yöntemler kullanılacaktır.\n\nÖdevler: Kısa ve öğrenciyi araştırmaya veya katılıma yönlendirecek veri odaklı görevler.\n\nVeri anketi (%10)\nDatacamp (Introduction to Tidyverse) (%20)\nYöneylem Araştırması Ödevi (%10)\n\nSınıf içi uygulamalar: Öğrencilerin kendi uzmanlık alanlarıyla, veri bilimi temellerini birleştirebilecekleri kısa süreli görevler. (%10)\nDerse katılım: Derste veri odaklı tartışmalara katılım ve misafir hocalar ile etkileşim (%10)\nDatacamp: Ders dışında teknik yetkinlikleri pekiştirme\n\nProje (%50)\n\nToplam 110 puan + Bonuslar"
  },
  {
    "objectID": "intro.html#grup-projesi",
    "href": "intro.html#grup-projesi",
    "title": "Tanışma ve Giriş",
    "section": "Grup Projesi",
    "text": "Grup Projesi\n\nGrupların oluşturulması (5-6 kişilik gruplar)\n\nGrup belirleme son tarih: 26 Nisan Çarşamba 23:59\n\nGrup ismi ve üyelerinin mail yoluyla bildirilmeli\n\nSon tarihe kadar gruplar oluşturulmadığı takdirde rastgele atama yapılacaktır\n\n\nGrup proje dökümanının Blackboard’a yüklenmesi\n\n\nhafta sunumlar\n\n\nGrup projeleri sektörel bir problem üzerinde yapılan bir araştırma, bir veri analizi ve çözüm önerilerinden oluşan bir sunum şeklinde yapılacaktır. Detaylar 2. hafta açıklanacaktır.\n## Etik\nVerimli ve adil bir ders deneyimi için çok basit kurallarımız bulunuyor.\n\nLütfen sınıf arkadaşlarınız ile beraber çalışın.\nLütfen internet ve diğer kaynakları çözüm bulmak için kullanın. Arada bir bu dersteki ödevler ve projeler hakkında çözümler bulma ihtimaliniz var ancak öğrenmek veya basitçe kopyala yapıştır yapmak size kalmış.\nLütfen sadece kopyala yapıştır yapmayın ve her zaman faydalandığınız kaynakları belirtin. Örneğin, “Bu linkteki görselleştirme tekniğini çok beğendim ve projemde uyguladım” her zaman için kabul edilebilir ve hatta istenen bir şeydir.\n\nAncak “Kaggle’da birinin yaptığı analizi boylu boyunca kopyalayıp yapıştırdım ve ödev olarak teslim ettim” demek kabul edilemez. Doğrudan F alma sebebidir.\n\nLütfen arkadaşlarınız ile bire bir aynı ödevi teslim etmeyin. Birbirinizin çalışmasından faydalanıyor olsanız dahi; aynı kodu ve aynı analizi, aynı cümleler ile vermeyin."
  },
  {
    "objectID": "intro.html#misafir-hocalar",
    "href": "intro.html#misafir-hocalar",
    "title": "Tanışma ve Giriş",
    "section": "Misafir Hocalar",
    "text": "Misafir Hocalar\n\nhafta ile 6. hafta arasında her hafta bir misafir hoca dersimize katılacak. Tahmini program aşağıdaki gibi gerçekleşecektir. Değişiklikler olabilir.\n\n\nBurak Yitgin - EPİAŞ Strateji Geliştirme / Sabancı Üniv Enerji Piyasaları\nErdem Sezer - Kalkınma Yatırım Bankası Sektörel Araştırma\nOnur Karadeli - SOCAR İleri Analitik\n(Yakında)\nGizem Gür - Sufle AWS Advanced Tier Partner\n\n## Veride el kirletme\n\nGerekli olabilecek programlar\nBook of EDA\nPosit Cloud\nGoogle Colab"
  },
  {
    "objectID": "intro.html#faydalı-kaynaklar",
    "href": "intro.html#faydalı-kaynaklar",
    "title": "Tanışma ve Giriş",
    "section": "Faydalı Kaynaklar",
    "text": "Faydalı Kaynaklar\n\nMEF BDA 503\n\nOR Primer\n\nIntroduction to Statistical Learning\n\nDatacamp\nHugging Face"
  },
  {
    "objectID": "intro_ml_1.html",
    "href": "intro_ml_1.html",
    "title": "Statistical Models in R: Part 1",
    "section": "",
    "text": "This is part of old lecture notes"
  },
  {
    "objectID": "intro_ml_1.html#properties-of-pca",
    "href": "intro_ml_1.html#properties-of-pca",
    "title": "Statistical Models in R: Part 1",
    "section": "Properties of PCA",
    "text": "Properties of PCA\n\nPCA is a method to reduce the number of explanatory variables. The resulting principle components (PC) are actually linear transformations of your explanatory variables. (e.g. \\(PC_i = a_1*x_1 + a_2*x_2 + \\dots + a_p*x_p\\)).\nIf some or most of your explanatory variables are highly correlated, then PCA is the way to go.\nTotal number of PCs are equal to total number of explanatory variables. Though, it is possible to eliminate some PCs without considerable loss in explanatory power.\nPCs are fairly independent (i.e. low correlation).\nYou cannot use categorical or binary variables in PCA.\nPCA is more of an exploration tool than an explanation or prediction tool. Though, it is possible to use PCs as input to other methods (i.e. regression).\nPCA considers linear relationship, so it is not good for non-linear relations (i.e. \\(x_1 = x_2^2\\)). It also assumes normally distributed errors, so fat tailed distributions are a poor fit to PCA.\nIt is possible to use covariance matrix rather than the correlation matrix.\nCentering and scaling can be done to get better results. Centering sets the mean values of the covariates to 0 and scaling converts explanatory variables to result in unit variance.\nIn R you can use both prcomp and princomp functions. While the former uses singular value decomposition method to calculate principle components, the latter uses eigenvalues and eigenvectors."
  },
  {
    "objectID": "intro_ml_1.html#examples",
    "href": "intro_ml_1.html#examples",
    "title": "Statistical Models in R: Part 1",
    "section": "Examples",
    "text": "Examples\n\nMade-up Example regarding Transformations\n\nset.seed(58)\n#Randomly create data points around the normal distribution\nx1=rnorm(30,mean=2,sd=4)\n#Get one linear transformation and one nonlinear transformation of the data\npca_data&lt;-data.frame(x1,x2=x1*2,x3=(x1^2),x4=abs(x1)^(0.5)+rnorm(30))\n#See the correlation matrix\npca_cor&lt;-cor(pca_data)\npca_cor\n\n           x1         x2        x3         x4\nx1 1.00000000 1.00000000 0.6648541 0.08208783\nx2 1.00000000 1.00000000 0.6648541 0.08208783\nx3 0.66485406 0.66485406 1.0000000 0.40646124\nx4 0.08208783 0.08208783 0.4064612 1.00000000\n\n#See the eigenvalues and eigenvectors\npca_eigen&lt;-eigen(pca_cor)\npca_eigen\n\neigen() decomposition\n$values\n[1] 2.625027e+00 1.067917e+00 3.070557e-01 2.492886e-16\n\n$vectors\n           [,1]       [,2]       [,3]          [,4]\n[1,] -0.5856026  0.2603411  0.2988179  7.071068e-01\n[2,] -0.5856026  0.2603411  0.2988179 -7.071068e-01\n[3,] -0.5269453 -0.2545741 -0.8108765  3.624189e-16\n[4,] -0.1909657 -0.8942243  0.4048395  1.129489e-16\n\n#See the standard deviations and proportion of variances\nsqrt(pca_eigen$values)\n\n[1] 1.620194e+00 1.033401e+00 5.541261e-01 1.578887e-08\n\npca_eigen$values/sum(pca_eigen$values)\n\n[1] 6.562569e-01 2.669792e-01 7.676393e-02 6.232214e-17\n\ncumsum(pca_eigen$values/sum(pca_eigen$values))\n\n[1] 0.6562569 0.9232361 1.0000000 1.0000000\n\n#Run PCA\npca_result&lt;-princomp(pca_data,cor=T)\n#See the PCA\nsummary(pca_result,loadings=TRUE)\n\nImportance of components:\n                          Comp.1    Comp.2     Comp.3 Comp.4\nStandard deviation     1.6201937 1.0334006 0.55412609      0\nProportion of Variance 0.6562569 0.2669792 0.07676393      0\nCumulative Proportion  0.6562569 0.9232361 1.00000000      1\n\nLoadings:\n   Comp.1 Comp.2 Comp.3 Comp.4\nx1  0.586  0.260  0.299  0.707\nx2  0.586  0.260  0.299 -0.707\nx3  0.527 -0.255 -0.811       \nx4  0.191 -0.894  0.405       \n\npca_result2&lt;-princomp(pca_data[,c(\"x1\",\"x3\",\"x4\")],cor=T)\nsummary(pca_result2,loadings=TRUE)\n\nImportance of components:\n                          Comp.1    Comp.2     Comp.3\nStandard deviation     1.3481348 0.9628649 0.50539453\nProportion of Variance 0.6058225 0.3090363 0.08514121\nCumulative Proportion  0.6058225 0.9148588 1.00000000\n\nLoadings:\n   Comp.1 Comp.2 Comp.3\nx1  0.601  0.517  0.609\nx3  0.690        -0.722\nx4  0.403 -0.855  0.327\n\n\n\n\nYoung People Survey\nFollowing data is from Kaggle. Data set is named Young People Survey. It is a simple survey with lots of questions and some demographic data. Resulting PCA analysis helps us to maintain 90% of the variance with just two-thirds of the survey questions.\n\n#Prepare data\nyr_data &lt;-\nread.csv(\"data/youth_responses.csv\",sep=\",\") %&gt;%\nfilter(complete.cases(.)) %&gt;%\n# mutate(id=row_number()) %&gt;%\ntbl_df()\n\nWarning: `tbl_df()` was deprecated in dplyr 1.0.0.\nℹ Please use `tibble::as_tibble()` instead.\n\n#Prepare PCA data\nyr_pca&lt;-\nyr_data[,sapply(yr_data,class)==\"integer\"] %&gt;%\nselect(Music:Spending.on.healthy.eating)\n\n#Run PCA analysis\nyr_pca_result&lt;-princomp(yr_pca,cor=T)\n\n#See the PCA output\nggplot(data=data.frame(PC=1:length(yr_pca_result$sdev),\n    var_exp=cumsum(yr_pca_result$sdev^2/sum(yr_pca_result$sdev^2))),\naes(x=PC,y=var_exp)) + geom_line() +\ngeom_point() +\nscale_y_continuous(labels = scales::percent,breaks=seq(0,1,length.out=11)) +\nscale_x_continuous(breaks=seq(0,135,by=5))"
  },
  {
    "objectID": "intro_ml_1.html#references",
    "href": "intro_ml_1.html#references",
    "title": "Statistical Models in R: Part 1",
    "section": "References",
    "text": "References\n\nhttp://www.rpubs.com/aaronsc32/principal-component-analysis\nhttps://onlinecourses.science.psu.edu/stat505/node/49\nhttp://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch18.pdf\nhttps://www.kaggle.com/c/santander-customer-satisfaction/data\nhttps://www.kaggle.com/c/santander-customer-satisfaction/data\nhttp://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf\nhttp://www.rpubs.com/koushikstat/pca\nhttps://www.kaggle.com/miroslavsabo/young-people-survey"
  },
  {
    "objectID": "intro_ml_1.html#properties",
    "href": "intro_ml_1.html#properties",
    "title": "Statistical Models in R: Part 1",
    "section": "Properties",
    "text": "Properties\n\nNon-parametric (except you need to specify the number of components).\nAnother dimension reduction technique.\nMDS is frequently used in psychology, sociology, archeology, biology, medicine, chemistry, network analysis and economics."
  },
  {
    "objectID": "intro_ml_1.html#references-1",
    "href": "intro_ml_1.html#references-1",
    "title": "Statistical Models in R: Part 1",
    "section": "References",
    "text": "References\n\nhttp://carme2011.agrocampus-ouest.fr/slides/Groenen.pdf"
  },
  {
    "objectID": "intro_ml_1.html#properties-1",
    "href": "intro_ml_1.html#properties-1",
    "title": "Statistical Models in R: Part 1",
    "section": "Properties",
    "text": "Properties\n\nK-Means is an unsupervised method. You don’t need a response variable or pre-labeled data.\nK-Means requires the number of clusters as input. Total error is a non-increasing function of number of clusters.\nK-Means is not suitable for every application.\nAs K-Means uses sum of squared errors, it is advised to scale the components. If component X1 ranges from 0 to 1 and component X2 ranges from -1000 to 1000, the algorithm will weigh considerably on X2.\nDiffering density, non-globular shapes, differing sizes all affect K-Means performance."
  },
  {
    "objectID": "intro_ml_1.html#properties-2",
    "href": "intro_ml_1.html#properties-2",
    "title": "Statistical Models in R: Part 1",
    "section": "Properties",
    "text": "Properties\n\nHierarchical clustering not only provides insight about the proximity of nodes, but also the proximity of sub-groups.\nDepending on the agglomeration method, cluster performance might vary significantly. MIN can handle non eliptical shapes but it is sensitive to noise and outliers. MAX handles outliers/noise better but it has a tendency to break large clusters and it is biased towards globular shapes. Average is a midway between MIN and MAX. Ward’s method is also better with noisy data and biased towards globular shape. It is also known to provide good starting centroids for K-Means.\nOnce agglomeration is done, it cannot be changed.\nWe don’t have a comprehensive objective function to optimize.\nMight not be suitable if data is too noisy, sparse or asymmetrical in size."
  },
  {
    "objectID": "intro_ml_1.html#references-2",
    "href": "intro_ml_1.html#references-2",
    "title": "Statistical Models in R: Part 1",
    "section": "References",
    "text": "References\n\nhttp://www3.nd.edu/~rjohns15/cse40647.sp14/www/content/lectures/13%20-%20Hierarchical%20Clustering.pdf"
  },
  {
    "objectID": "intro_ml_1.html#linear-regression",
    "href": "intro_ml_1.html#linear-regression",
    "title": "Statistical Models in R: Part 1",
    "section": "Linear Regression",
    "text": "Linear Regression\nLinear regression is the most basic kind of regression. The structure is quite easy to comprehend (\\(Y = \\beta_0 + \\beta_1*X_1 + \\dots + \\beta_k*X_k\\)).\nFollowing example uses swiss data (included in base R). Data consists of fertility measure and socio-economic indicators of 47 French-speaking provinces of Switzerland at 1888. These indicators are agriculture (percentage of males work in agriculture), examination (percentage of highest marks in army examination), education (percentage of people who received education beyond primary school), catholic (percentage of catholic people) and infant mortality (as percentage of babies who live less than 1 year).\n\n#First check the data\nhead(swiss)\n\n             Fertility Agriculture Examination Education Catholic\nCourtelary        80.2        17.0          15        12     9.96\nDelemont          83.1        45.1           6         9    84.84\nFranches-Mnt      92.5        39.7           5         5    93.40\nMoutier           85.8        36.5          12         7    33.77\nNeuveville        76.9        43.5          17        15     5.16\nPorrentruy        76.1        35.3           9         7    90.57\n             Infant.Mortality\nCourtelary               22.2\nDelemont                 22.2\nFranches-Mnt             20.2\nMoutier                  20.3\nNeuveville               20.6\nPorrentruy               26.6\n\n#Run a simple linear regression\nggplot(data=swiss) + geom_point(aes(x=Examination,y=Fertility)) +\n geom_smooth(method='lm',aes(x=Examination,y=Fertility),se=FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nfert_vs_exam&lt;-lm(Fertility ~ Examination, data=swiss)\nsummary(fert_vs_exam)\n\n\nCall:\nlm(formula = Fertility ~ Examination, data = swiss)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-25.9375  -6.0044  -0.3393   7.9239  19.7399 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  86.8185     3.2576  26.651  &lt; 2e-16 ***\nExamination  -1.0113     0.1782  -5.675 9.45e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.642 on 45 degrees of freedom\nMultiple R-squared:  0.4172,    Adjusted R-squared:  0.4042 \nF-statistic: 32.21 on 1 and 45 DF,  p-value: 9.45e-07\n\n#Now Fertility vs all\nfert_vs_all&lt;-lm(Fertility ~ ., data=swiss)\nsummary(fert_vs_all)\n\n\nCall:\nlm(formula = Fertility ~ ., data = swiss)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2743  -5.2617   0.5032   4.1198  15.3213 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      66.91518   10.70604   6.250 1.91e-07 ***\nAgriculture      -0.17211    0.07030  -2.448  0.01873 *  \nExamination      -0.25801    0.25388  -1.016  0.31546    \nEducation        -0.87094    0.18303  -4.758 2.43e-05 ***\nCatholic          0.10412    0.03526   2.953  0.00519 ** \nInfant.Mortality  1.07705    0.38172   2.822  0.00734 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.165 on 41 degrees of freedom\nMultiple R-squared:  0.7067,    Adjusted R-squared:  0.671 \nF-statistic: 19.76 on 5 and 41 DF,  p-value: 5.594e-10\n\n\nFertility can seemingly be explained by those covariates. But what about classification problems? Linear regression is not very well suited in categorical response variables."
  },
  {
    "objectID": "intro_ml_1.html#logistic-regression",
    "href": "intro_ml_1.html#logistic-regression",
    "title": "Statistical Models in R: Part 1",
    "section": "Logistic regression",
    "text": "Logistic regression\nLogistic regression is commonly used in binary response variables such as high/low, accept/reject, yes/no type situations. In the following example we will build a credit scoring model which will “tell” us if a demanded loan is good or bad. We are going to use the German Credit data (download from here). It consists of credit applications and classified as accepted/rejected. The other 20 columns are explanatory variables such as account balance, occupation, gender and marital status, savings, purpose of the credit, payment status. For our example we will use only three covariates (account balance, occupation sex/marital status).\nCovariates are score variables (can also be considered as categorical). Account.Balance (1: no running account, 2: no balance or debit, 3: 0 &lt;= … &lt; 200 DM, 4: … &gt;= 200 DM or checking account for at least 1 year) and Occupation (1: unemployed / unskilled with no permanent residence, 2: unskilled with permanent residence, 3: skilled worker / skilled employee / minor civil servant, 4: executive / self-employed / higher civil servant).\n\nset.seed(58)\n##Get the data\ncredit_data&lt;-read.csv(\"data/german_credit.csv\") %&gt;%\nselect(Creditability, Account.Balance, Occupation) %&gt;%\ntbl_df()\n\nprint(head(credit_data))\n\n# A tibble: 6 × 3\n  Creditability Account.Balance Occupation\n          &lt;int&gt;           &lt;int&gt;      &lt;int&gt;\n1             1               1          3\n2             1               1          3\n3             1               2          2\n4             1               1          2\n5             1               1          2\n6             1               1          2\n\n##Use 50% as training set\ntrain_sample&lt;-sample(1:nrow(credit_data),round(nrow(credit_data)/2),replace=FALSE)\ncredit_train &lt;- credit_data %&gt;% slice(train_sample)\ncredit_test &lt;- credit_data %&gt;% slice(-train_sample)\n\n#Assume covariates are scores\ncredit_model&lt;-glm(Creditability ~ Account.Balance + Occupation,\n    family=binomial(link=\"logit\"),data=credit_train)\n#See the summary of the model\nsummary(credit_model)\n\n\nCall:\nglm(formula = Creditability ~ Account.Balance + Occupation, family = binomial(link = \"logit\"), \n    data = credit_train)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.1308  -1.0596   0.4936   0.9318   1.3518  \n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -0.70704    0.50122  -1.411    0.158    \nAccount.Balance  0.77575    0.09165   8.464   &lt;2e-16 ***\nOccupation      -0.11746    0.16161  -0.727    0.467    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 628.37  on 499  degrees of freedom\nResidual deviance: 541.02  on 497  degrees of freedom\nAIC: 547.02\n\nNumber of Fisher Scoring iterations: 4\n\n#Check the predictions for both in sample and out of sample data\ncredit_model_in_sample&lt;-\ndata.frame(actual=credit_train$Creditability,\n    fitted=predict(credit_model,type=\"response\"))\ncredit_model_in_sample %&gt;%\n    group_by(actual) %&gt;%\n    summarise(mean_pred=mean(fitted),sd_pred=sd(fitted))\n\n# A tibble: 2 × 3\n  actual mean_pred sd_pred\n   &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1      0     0.566   0.160\n2      1     0.731   0.181\n\ncredit_model_out_of_sample&lt;-\ndata.frame(actual=credit_test$Creditability,\n    fitted=predict(credit_model,newdata=credit_test,type=\"response\"))\ncredit_model_out_of_sample %&gt;%\n    group_by(actual) %&gt;%\n    summarise(mean_pred=mean(fitted),sd_pred=sd(fitted))\n\n# A tibble: 2 × 3\n  actual mean_pred sd_pred\n   &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1      0     0.605   0.166\n2      1     0.725   0.181\n\n#Model as covariates are assumed as ordered factors\ncredit_model_fctr&lt;-\n    glm(Creditability ~ ordered(Account.Balance,levels=1:4) +\n    ordered(Occupation,levels=1:4),\n     family=binomial,data=credit_train)\n#See the summary of the model\nsummary(credit_model_fctr)\n\n\nCall:\nglm(formula = Creditability ~ ordered(Account.Balance, levels = 1:4) + \n    ordered(Occupation, levels = 1:4), family = binomial, data = credit_train)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.2071  -1.0514   0.4946   0.9303   1.3089  \n\nCoefficients:\n                                         Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)                               1.01555    0.21439   4.737 2.17e-06\nordered(Account.Balance, levels = 1:4).L  1.75025    0.21852   8.009 1.15e-15\nordered(Account.Balance, levels = 1:4).Q  0.15995    0.27455   0.583    0.560\nordered(Account.Balance, levels = 1:4).C -0.01146    0.32176  -0.036    0.972\nordered(Occupation, levels = 1:4).L      -0.25892    0.48012  -0.539    0.590\nordered(Occupation, levels = 1:4).Q       0.15076    0.38067   0.396    0.692\nordered(Occupation, levels = 1:4).C       0.14096    0.24315   0.580    0.562\n                                            \n(Intercept)                              ***\nordered(Account.Balance, levels = 1:4).L ***\nordered(Account.Balance, levels = 1:4).Q    \nordered(Account.Balance, levels = 1:4).C    \nordered(Occupation, levels = 1:4).L         \nordered(Occupation, levels = 1:4).Q         \nordered(Occupation, levels = 1:4).C         \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 628.37  on 499  degrees of freedom\nResidual deviance: 539.49  on 493  degrees of freedom\nAIC: 553.49\n\nNumber of Fisher Scoring iterations: 4\n\n#Check the predictions for both in sample and out of sample data\ncredit_model_in_sample&lt;-\n    data.frame(actual=credit_train$Creditability,\n        fitted=predict(credit_model_fctr,type=\"response\"))\ncredit_model_in_sample %&gt;% group_by(actual) %&gt;%\n    summarise(mean_pred=mean(fitted),sd_pred=sd(fitted))\n\n# A tibble: 2 × 3\n  actual mean_pred sd_pred\n   &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1      0     0.565   0.157\n2      1     0.732   0.182\n\ncredit_model_out_of_sample&lt;-\n    data.frame(actual=credit_test$Creditability,\n        fitted=predict(credit_model_fctr,newdata=credit_test,type=\"response\"))\n\ncredit_model_out_of_sample %&gt;%\n    group_by(actual) %&gt;%\n    summarise(mean_pred=mean(fitted),sd_pred=sd(fitted))\n\n# A tibble: 2 × 3\n  actual mean_pred sd_pred\n   &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1      0     0.606   0.166\n2      1     0.725   0.181"
  },
  {
    "objectID": "intro_ml_1.html#k-nearest-neighbors-knn",
    "href": "intro_ml_1.html#k-nearest-neighbors-knn",
    "title": "Statistical Models in R: Part 1",
    "section": "K Nearest Neighbors (KNN)",
    "text": "K Nearest Neighbors (KNN)\nThe name explains everything. For each node, the algorithm checks the \\(k\\) nearest nodes (by euclidean distance of covariates). Based on their “votes”, the class of the node is estimated. If there is a tie,\n\n##We are going to use knn function of the class package\n## If the class package is not loaded use install.packages(\"class\")\n##Below example is from iris data set\n## train is the covariates of the training set\ntrain &lt;- rbind(iris3[1:25,,1], iris3[1:25,,2], iris3[1:25,,3])\n## cl is the class of the training set\n## First 25 nodes are (s)etosa, second 25 are versi(c)olor, third 25 are (v)irginica\ncl &lt;- factor(c(rep(\"s\",25), rep(\"c\",25), rep(\"v\",25)))\n## test is the test set we want to predict its classes\ntest &lt;- rbind(iris3[26:50,,1], iris3[26:50,,2], iris3[26:50,,3])\n##The following function uses the train set and classes to predict\n#the class of the test set's classes\nknn_cl&lt;-class::knn(train, test, cl, k = 3, prob=TRUE)\nknn_cl\n\n [1] s s s s s s s s s s s s s s s s s s s s s s s s s c c v c c c c c v c c c c\n[39] c c c c c c c c c c c c v c c v v v v v c v v v v c v v v v v v v v v v v\nattr(,\"prob\")\n [1] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000\n [8] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000\n[15] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000\n[22] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 0.6666667\n[29] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 0.6666667 1.0000000\n[36] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000\n[43] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000\n[50] 1.0000000 1.0000000 0.6666667 0.7500000 1.0000000 1.0000000 1.0000000\n[57] 1.0000000 1.0000000 0.5000000 1.0000000 1.0000000 1.0000000 1.0000000\n[64] 0.6666667 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000\n[71] 1.0000000 0.6666667 1.0000000 1.0000000 0.6666667\nLevels: c s v\n\n##Let's also compare knn estimates vs actual test data classes (same order as train data)\ntable(data.frame(actual=cl,estimate=knn_cl))\n\n      estimate\nactual  c  s  v\n     c 23  0  2\n     s  0 25  0\n     v  4  0 21"
  },
  {
    "objectID": "intro_ml_1.html#references-3",
    "href": "intro_ml_1.html#references-3",
    "title": "Statistical Models in R: Part 1",
    "section": "References",
    "text": "References\n\nThese lecture notes were initially used at Bogazici University Engineering Management Master Program.\nhttps://onlinecourses.science.psu.edu/stat857/node/215"
  },
  {
    "objectID": "proje.html",
    "href": "proje.html",
    "title": "Grup Projeleri",
    "section": "",
    "text": "Bu sayfa grup projelerinin yöntemini, değerlendirme kurallarını ve takvimini içermektedir. En son güncelleme: 20 Nisan 10:00."
  },
  {
    "objectID": "proje.html#proje-konusu",
    "href": "proje.html#proje-konusu",
    "title": "Grup Projeleri",
    "section": "Proje Konusu",
    "text": "Proje Konusu\n\nHer grubun iki alternatifi bulunmaktadır.\n\nKendi çalışmalarını içeren bir veri projesi yapabilirler.\nTÜİK Data Portal üzerinde sunulan veri setlerinden birini kullanarak bir proje yapabilirler.\n\nAnalizleri yaparken kod kullanmak zorunlu değil (Excel veya başka programlar da serbest) ama kullanmak tavsiye edilir.\nProjenin konusu ne olursa olsun tatmin edici bir şekilde veri analizleri yapılması gerekmektedir. Ancak sunumların ana teması bulguların iletişimidir. Diğer bir deyişle, kullandığınız tekniklerin detaylarından çok çıktılarınıza dikkat ediyoruz. Bir üst yönetim toplantısında çalışmalarınızı ve sonuçlarınızı anlatır gibi düşünebilirsiniz."
  },
  {
    "objectID": "proje.html#sunum-günü-24-mayıs",
    "href": "proje.html#sunum-günü-24-mayıs",
    "title": "Grup Projeleri",
    "section": "Sunum Günü (24 Mayıs)",
    "text": "Sunum Günü (24 Mayıs)\n\n\nHafta (son ders, 24 Mayıs) sunumlara ayrılmış bulunmaktadır. 2 oturum olacak (4 + 4) ve sunumlar 15:10’da başlayacaktır.\n\nHer grubun 12 dakika sunum + 3 dakika soru cevap olmak üzere 15 dakikalık süresi vardır.\nHer gruptan sadece 1 kişinin sunması yeterlidir ama kişi sayısında sınır bulunmamaktadır."
  },
  {
    "objectID": "proje.html#sunum-provası-18-mayıs",
    "href": "proje.html#sunum-provası-18-mayıs",
    "title": "Grup Projeleri",
    "section": "Sunum provası (18 Mayıs)",
    "text": "Sunum provası (18 Mayıs)\nSunum gününden önceki perşembeye (18 Mayıs) kadar sunum provanızı teslim etmeniz gerekmektedir. Sunum provası, sunumunuzu içeren max 12 dakikalık bir video kaydından oluşmalıdır.\nSunumunuzun son hali olmayabilir ancak son hale yakın olması tercih edilir. Sunum provaları eposta ile dosya formatına teslim edilecektir (değişebilir). Her gruptan bir kişinin teslim etmesi yeterlidir."
  },
  {
    "objectID": "proje.html#sunumlar-ve-teslim",
    "href": "proje.html#sunumlar-ve-teslim",
    "title": "Grup Projeleri",
    "section": "Sunumlar ve Teslim",
    "text": "Sunumlar ve Teslim\n\nHer sunumun başındaki slaytlardan biri 3-5 kısa (1-1,5 cümle) maddeden oluşan “Önemli Çıkarımlar” slaytı olmalı.\nSlayt sayısı her şey dahil (giriş ve teşekkürler de dahil) 20yi aşmamalıdır. 20nin üzerindeki her ek slayt -10 puan. Sunumun sonunda sorulara göre gösterilebilecek veya daha sonra değerlendirilmesi istenen Ekler kısmı slaytları yer alabilir (“Ekler” diye bir ara slayt ile ayırmalısınız).\nSunumlarda kod göstermek zorunda değilsiniz. Hatta minimal seviyede tutulması tavsiye edilir.\nDeğerlendirme veri projesinin işlenmesi kadar, sunum ve iletişim kalitesi ile de yapılacak. Örneğin slaytları anlamsız karmaşık grafikler ve okunamayan veri tablolarıyla doldurmak kötü değerlendirmeye sebep olur. Teknik kabiliyetinizden eminiz ama problem tanımı, metodoloji, sonuçlar ve çıkarımlar her zaman için net olmalı.\nSunumu yapan kişi sunarken kamerasını açmalıdır.\nSunumlar (Powerpoint dosyaları) Blackboard’a yüklenecektir (ilgili yükleme sayfası son hafta aktif olacaktır). Son tarih gene sunum günü olan 24 Mayıs’tır."
  },
  {
    "objectID": "proje.html#değerlendirme",
    "href": "proje.html#değerlendirme",
    "title": "Grup Projeleri",
    "section": "Değerlendirme",
    "text": "Değerlendirme\nDeğerlendirme 100 puan üzerinden yapılacaktır.\n\nSunum provası: 20 puan\nProje konusu ve veri setinin yeterliliği: 20 puan\nTeknik yetkinlik ve analiz araçları: 15 puan\nSunum görsel kalitesi: 10 puan\nSunum akışı ve analizin iletişimi: 35 puan (süreye uyum dahil)\n\nDeğerlendirmeyi negatif etkileyebilecek teknik faktörler\n\n20nin üzerindeki slayt sayısı (ekler hariç): -10 puan / slayt\n12 dakikanın üzerindeki süre: -10 puan / dakika"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MEF MGMT 553 Bahar 2022-2023",
    "section": "",
    "text": "Projeler\n\n\nDers 7 (24 Mayıs 2023)\n\nSunumlar!\n\n\n\nDers 6 (17 Mayıs 2023)\n\nMisafir hoca: Gizem Gür Kalkınma Yatırım Bankası Sektörel Araştırma Kıdemli Başkan Yardımcısı\nBulut Bilişim\n\nTürkçe Kaynak\n\n\n\n\nDers 5 (10 Mayıs 2023)\n\nMisafir hoca: Erdem Sezer Kalkınma Yatırım Bankası Sektörel Araştırma Kıdemli Başkan Yardımcısı\nMakine Öğrenmesi (devam)\nÜrün Geliştirme / Ürün Yönetimi\n\nTürkçe Kaynak | Türkçe Kaynak 2\n\n\n\n\nDers 4 (3 Mayıs 2023)\n\nMisafir hoca: Onur Karadeli İleri Analitik, RPA ve Büyük Veri Müdürü, Socar Türkiye\nKonu: Makine Öğrenmesi, Yapay Zeka ve Optimizasyon Giriş\n\nOptimizasyon Giriş\n\nKaynak\nEk Kaynak\nTürkçe Kaynak | Türkçe Kaynak 2\n\nMakine Öğrenmesi ve Yapay Zeka Giriş\n\nMulti Armed Bandits\n\nTürkçe Kaynak\n\nMakine Öğrenmesi Giriş - 1 | Makine Öğrenmesi Giriş - 2\n\nTürkçe Kaynak - Çoklu Doğrusal Regresyon\nTürkçe Kaynak - Makine Öğrenmesi\nTürkçe Kaynak - Karar Ağaçları\n\nEk Kaynak\n\n\n\n\n\nDers 3 (26 Nisan 2023)\n\nKonu: Dinamik Raporlama (Quarto / RMarkdown)\n\nEk Kaynak\nTürkçe Kaynak\nTürkçe Kaynak 2 - RMarkdown\n\nKonu 2: Dashboard oluşturma (Shiny)\n\nTürkçe Kaynak\n\nSerbest Uygulama: YÖK Yabancı Öğrenci Verisi\n\nBaşlangıç Kodu\n\n\n\n\nDers 2 (19 Nisan 2023)\n\nMisafir hoca: Burak Yitgin EPİAŞ Strateji Geliştirme Kıdemli Uzmanı ve Akademisyen (Sabancı Üniversitesi - Enerji Teknolojileri ve Yönetimi YL)\nKonu: Veri Düzenleme ve Görselleştirme\n\ndplyr: Kaynak - Ek kaynak\nggplot2: Kaynak - Ek kaynak\n\nSerbest Uygulama: YÖK Yabancı Öğrenci Verisi\n\nBaşlangıç Kodu\n\n\n\n\nDers 1 (12 Nisan 2023)\n\nGiriş\nÖdev 1: Tanışma (Blackboard)\nUygulama: Startup Deals 2021\n\nExcel üzerinde bu veriyi inceleyip düzenleyip bir “rapor” hazırlayın\nRapor Powerpoint formatında olsun max 5 sayfa\n\n\nEk kaynak:\n\nMEF BDA 503\nEskişehir R User Group\n\nThis class is supported by Datacamp for Universities. Students will learn from interactive environments in their journeys as data analysts and data scientists."
  }
]